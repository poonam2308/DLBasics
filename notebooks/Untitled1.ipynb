{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "09d11812-c728-4f83-be5c-f9b9df2b8317",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 10, 128])\n",
      "torch.Size([32, 10, 128])\n",
      "torch.Size([8, 10])\n",
      "torch.Size([20, 10, 128])\n"
     ]
    }
   ],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FeedForwardNetwork(nn.Module):\n",
    "    def __init__(self, embed_dim, ff_dim):\n",
    "        super(FeedForwardNetwork, self).__init__()\n",
    "        # Define each layer separately\n",
    "        self.fc1 = nn.Linear(embed_dim, ff_dim)\n",
    "        self.relu = nn.ReLU()\n",
    "        self.fc2 = nn.Linear(ff_dim, embed_dim)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # Forward pass through each layer\n",
    "        x = self.fc1(x)      # First linear layer\n",
    "        x = self.relu(x)     # ReLU activation\n",
    "        x = self.fc2(x)      # Second linear layer\n",
    "        return x\n",
    "\n",
    "\n",
    "# Example usage\n",
    "embed_dim = 128\n",
    "ff_dim = 512\n",
    "ffn = FeedForwardNetwork(embed_dim, ff_dim)\n",
    "\n",
    "# Example input tensor of shape (batch_size, seq_length, embed_dim)\n",
    "x = torch.randn(32, 10, embed_dim)  # Batch of 32 samples, each with a sequence length of 10\n",
    "\n",
    "# Pass the input through the feed-forward network\n",
    "output = ffn(x)\n",
    "\n",
    "print(output.shape)  # Should be (32, 10, embed_dim)import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class MultiHeadSelfAttention(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads):\n",
    "        super(MultiHeadSelfAttention, self).__init__()\n",
    "        self.embed_dim = embed_dim\n",
    "        self.num_heads = num_heads\n",
    "        self.head_dim = embed_dim // num_heads\n",
    "\n",
    "        assert self.head_dim * num_heads == embed_dim, \"Embedding dimension must be divisible by number of heads\"\n",
    "\n",
    "        self.qkv = nn.Linear(embed_dim, embed_dim * 3)\n",
    "        self.out = nn.Linear(embed_dim, embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        N, L, E = x.shape\n",
    "\n",
    "        qkv = self.qkv(x)  # (N, L, 3*E)\n",
    "        qkv = qkv.reshape(N, L, 3, self.num_heads, self.head_dim) #(N, L, 3, num_heds, head_dim)\n",
    "        qkv = qkv.permute(2,0,3,1,4) # (3, N, num_heads, L, head_dim)\n",
    "\n",
    "        q, k , v = qkv[0], qkv[1], qkv[2]\n",
    "        attn_weights = torch.matmul(q, k.transpose(-2,-1)) # (N, num_heads, L, L)\n",
    "        attn_weights = attn_weights / np.sqrt(self.head_dim)\n",
    "        attn_weights = torch.nn.functional.softmax(attn_weights, dim=-1)\n",
    "\n",
    "        out = torch.matmul(attn_weights, v) # (N, num_heads, L, head_dim)\n",
    "        out = out.transpose(1,2).reshape(N,L,E) # (N, L, E)\n",
    "        out = self.out(out)\n",
    "\n",
    "        return out\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "\n",
    "class SimpleTransformerLayer(nn.Module):\n",
    "    def __init__(self, embed_dim, num_heads, ff_dim):\n",
    "        super(SimpleTransformerLayer, self).__init__()\n",
    "        self.attention = MultiHeadSelfAttention(embed_dim, num_heads)\n",
    "        self.ffn = FeedForwardNetwork(embed_dim, ff_dim)\n",
    "        self.norm1 = nn.LayerNorm(embed_dim)\n",
    "        self.norm2 = nn.LayerNorm(embed_dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Multi-head self-attention\n",
    "        attn_output = self.attention(x)\n",
    "        x = self.norm1(x + attn_output)\n",
    "\n",
    "        # Feed-forward network\n",
    "        ffn_output = self.ffn(x)\n",
    "        x = self.norm2(x + ffn_output)\n",
    "\n",
    "        return x\n",
    "\n",
    "# Instantiate the transformer layer\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "transformer_layer = SimpleTransformerLayer(embed_dim, num_heads, ff_dim)\n",
    "\n",
    "# Example input (batch size N, sequence length L, embedding dimension E)\n",
    "x = torch.randn(32, 10, embed_dim)  # Batch of 32, sequence length of 10, embedding dim of 128\n",
    "\n",
    "# Pass the input through the transformer layer\n",
    "output = transformer_layer(x)\n",
    "\n",
    "print(output.shape)  # Should be (32, 10, 128)\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(self, img_size, patch_size, embed_dim, num_heads, ff_dim, num_classes):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "        \n",
    "        self.patch_size = patch_size\n",
    "        self.embed_dim = embed_dim\n",
    "        \n",
    "        # Create patches and project them into embedding space\n",
    "        self.patch_embedding = nn.Conv2d(3, embed_dim, kernel_size=patch_size, stride=patch_size)\n",
    "        \n",
    "        # Classification token\n",
    "        self.cls_token = nn.Parameter(torch.randn(1, 1, embed_dim))\n",
    "        \n",
    "        # Positional embeddings\n",
    "        num_patches = (img_size // patch_size) ** 2\n",
    "        self.positional_embedding = nn.Parameter(torch.randn(num_patches + 1, embed_dim))\n",
    "        \n",
    "        # Transformer encoder layers\n",
    "        encoder_layer = nn.TransformerEncoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "        self.transformer_encoder = nn.TransformerEncoder(encoder_layer, num_layers=1)  # Simplified to 1 layer for demonstration\n",
    "        \n",
    "        # Output classification head\n",
    "        self.fc = nn.Linear(embed_dim, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        # Extract patches and project them to embedding space\n",
    "        x = self.patch_embedding(x)  # Shape: (N, embed_dim, H', W')\n",
    "        x = x.flatten(2)  # Shape: (N, embed_dim, num_patches)\n",
    "        x = x.transpose(1, 2)  # Shape: (N, num_patches, embed_dim)\n",
    "        \n",
    "        # Add cls_token\n",
    "        batch_size = x.size(0)\n",
    "        cls_tokens = self.cls_token.expand(batch_size, -1, -1)  # Shape: (N, 1, embed_dim)\n",
    "        x = torch.cat([cls_tokens, x], dim=1)  # Shape: (N, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # Add positional embeddings\n",
    "        x = x + self.positional_embedding  # Shape: (N, num_patches + 1, embed_dim)\n",
    "        \n",
    "        # Reorder dimensions for TransformerEncoder (sequence_length, batch_size, embed_dim)\n",
    "        x = x.transpose(0, 1)  # Shape: (num_patches + 1, N, embed_dim)\n",
    "        \n",
    "        # Pass through transformer encoder\n",
    "        x = self.transformer_encoder(x)  # Shape: (num_patches + 1, N, embed_dim)\n",
    "        \n",
    "        # Extract the output corresponding to cls_token\n",
    "        x = x.transpose(0, 1)  # Shape: (N, num_patches + 1, embed_dim)\n",
    "        cls_output = x[:, 0]  # Shape: (N, embed_dim)\n",
    "        \n",
    "        # Classification head\n",
    "        logits = self.fc(cls_output)  # Shape: (N, num_classes)\n",
    "        return logits\n",
    "\n",
    "# Example usage\n",
    "img_size = 32\n",
    "patch_size = 8\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "ff_dim = 512\n",
    "num_classes = 10\n",
    "\n",
    "# Create the Vision Transformer model\n",
    "model = VisionTransformer(img_size, patch_size, embed_dim, num_heads, ff_dim, num_classes)\n",
    "\n",
    "# Example input tensor: batch of 32x32 RGB images\n",
    "x = torch.randn(8, 3, img_size, img_size)  # Batch size of 8\n",
    "\n",
    "# Forward pass\n",
    "logits = model(x)\n",
    "\n",
    "print(logits.shape)  # Should be (8, num_classes), i.e., (8, 10)\n",
    "\n",
    "\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "# Define parameters\n",
    "embed_dim = 128\n",
    "num_heads = 8\n",
    "num_layers = 6\n",
    "ff_dim = 512\n",
    "batch_size = 10\n",
    "seq_len_tgt = 20  # Length of the target sequence\n",
    "seq_len_mem = 15  # Length of the memory sequence\n",
    "\n",
    "# Create a single TransformerDecoderLayer\n",
    "decoder_layer = nn.TransformerDecoderLayer(d_model=embed_dim, nhead=num_heads, dim_feedforward=ff_dim)\n",
    "\n",
    "# Create a TransformerDecoder with multiple layers\n",
    "transformer_decoder = nn.TransformerDecoder(decoder_layer, num_layers=num_layers)\n",
    "\n",
    "# Example memory and target tensors\n",
    "# Memory tensor: shape should be (S, N, E) where S is the source sequence length\n",
    "memory = torch.randn(seq_len_mem, batch_size, embed_dim)  # (S, N, E)\n",
    "\n",
    "# Target tensor: shape should be (T, N, E) where T is the target sequence length\n",
    "tgt = torch.randn(seq_len_tgt, batch_size, embed_dim)     # (T, N, E)\n",
    "\n",
    "# Pass tensors through the TransformerDecoder\n",
    "output = transformer_decoder(tgt, memory)\n",
    "\n",
    "print(output.shape)  # Output shape should be (T, N, E)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "55b8fbd4-d58e-4523-b527-5e698f4c6c86",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_t = torch.randn(1,1,4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "9a427e5e-1c14-4feb-831d-71897bbaa4cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2666,  1.0577, -0.3493,  0.0475]]])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "a564f6b3-5006-4b4a-9343-82bf4eb21746",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "4bfd16ba-356b-4ebd-b8d7-400a5c6cc8e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "cls_t =cls_t.expand(N, -1,-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "960c8437-e5b2-468f-87f2-e2467c567911",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[-0.2666,  1.0577, -0.3493,  0.0475]],\n",
       "\n",
       "        [[-0.2666,  1.0577, -0.3493,  0.0475]],\n",
       "\n",
       "        [[-0.2666,  1.0577, -0.3493,  0.0475]]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "73e8ee7d-c46a-4f68-963e-9ffc29a666a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([3, 1, 4])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cls_t.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "2a6e5119-e0e9-4ba7-9e79-6d885fa0bb91",
   "metadata": {},
   "outputs": [],
   "source": [
    "b= torch.randn(1,1,3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "f2dd740e-62a8-458b-97bd-fe8aa3fc39b5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[[0.2983, 0.5095, 0.1856],\n",
       "         [0.2983, 0.5095, 0.1856]],\n",
       "\n",
       "        [[0.2983, 0.5095, 0.1856],\n",
       "         [0.2983, 0.5095, 0.1856]],\n",
       "\n",
       "        [[0.2983, 0.5095, 0.1856],\n",
       "         [0.2983, 0.5095, 0.1856]]])"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "b.expand(N, 2, -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8244fc9d-c5ff-45b2-b350-d8524e7be841",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
